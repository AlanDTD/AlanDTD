# Planned Outage Analysis for Sentiment Scores

This document provides Python code snippets to analyse sentiment scores for planned outages by days of the week, timing, and customer verbatims. The goal is to identify optimal scheduling and customer feedback themes to improve customer sentiment.

## 1. Data Preparation
Ensure your data frame `df` contains:
- `date`: date of outage
- `sentiment_score`: customer sentiment score
- `time`: outage time (morning, afternoon, evening)
- `verbatim`: customer verbatim feedback

```python
import pandas as pd
import matplotlib.pyplot as plt

# Convert date column and extract weekday and time period
df['date'] = pd.to_datetime(df['date'])
df['day_of_week'] = df['date'].dt.day_name()
```

## 2. Sentiment by Day and Time
Calculate and plot sentiment scores by day and outage timing:

```python
sentiment_day_time = df.groupby(['day_of_week', 'time'])['sentiment_score'].mean().unstack()
sentiment_day_time = sentiment_day_time.reindex(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])

sentiment_day_time.plot(kind='bar')
plt.xlabel('Day of Week')
plt.ylabel('Avg Sentiment')
plt.title('Average Sentiment by Day and Time')
plt.tight_layout()
plt.show()
```
**Look out for:**
- Identify days and times with consistently higher or lower sentiment scores.

## 3. Statistical Testing for Optimal Timing
Conduct ANOVA to determine if sentiment significantly differs by time:

```python
from scipy.stats import f_oneway

morning_scores = df[df['time']=='morning']['sentiment_score']
afternoon_scores = df[df['time']=='afternoon']['sentiment_score']
evening_scores = df[df['time']=='evening']['sentiment_score']

f_stat, p_val = f_oneway(morning_scores, afternoon_scores, evening_scores)
print(f'F-stat: {f_stat}, p-value: {p_val}')
```
**Look out for:**
- p-values < 0.05 indicating a significant difference in sentiment by time.

## 4. Predictive Modelling with OLS (Day and Time)
Evaluate the impact of days and timing on sentiment scores:

```python
import statsmodels.formula.api as smf
model_time = smf.ols('sentiment_score ~ C(day_of_week) + C(time)', data=df).fit()
print(model_time.summary())
```
**Look out for:**
- Significant coefficients showing the specific impact of timing and day.

## 5. Predictive Uplift Analysis
Estimate improvements by changing day or time (e.g., moving outages to optimal times):

```python
predicted_improvement = model_time.predict(df.assign(day_of_week='Wednesday', time='afternoon')) - model_time.predict(df)
print('Avg predicted uplift if moved to Wednesday afternoon:', predicted_improvement.mean())
```
**Look out for:**
- Positive uplift values showing potential improvements in sentiment.

## 6. Random Forest Regression Validation
Further validate findings using machine learning:

```python
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split

pipeline_rf = make_pipeline(OneHotEncoder(), RandomForestRegressor(n_estimators=100, random_state=42))
X = df[['day_of_week', 'time']]
y = df['sentiment_score']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pipeline_rf.fit(X_train, y_train)
print('RÂ² score:', pipeline_rf.score(X_test, y_test))
```

## 7. Predictive Uplift with Random Forest
Confirm potential improvements:

```python
uplift_rf = (pipeline_rf.predict(pd.DataFrame({'day_of_week': ['Wednesday']*len(df), 'time': ['afternoon']*len(df)})) - pipeline_rf.predict(X)).mean()
print('Avg uplift (RF) if moved to Wednesday afternoon:', uplift_rf)
```

## 8. Individual Day and Time Comparison (Pairwise Testing)
Use Tukey's HSD test to statistically compare days and times:

```python
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# Pairwise comparisons for time
tukey_time = pairwise_tukeyhsd(df['sentiment_score'], df['time'], alpha=0.05)
print(tukey_time)

# Pairwise comparisons for day
tukey_day = pairwise_tukeyhsd(df['sentiment_score'], df['day_of_week'], alpha=0.05)
print(tukey_day)
```
**Look out for:**
- Significant differences guiding optimal scheduling choices.

## 9. Verbatim Analysis (Text Processing)
Analyse customer verbatims for thematic insights:

```python
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Word Cloud
text = ' '.join(df['verbatim'].dropna().values)
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Customer Feedback Word Cloud')
plt.show()

# Common phrases
vectorizer = CountVectorizer(ngram_range=(2,3), stop_words='english')
X_counts = vectorizer.fit_transform(df['verbatim'].dropna())
phrase_counts = pd.DataFrame(X_counts.sum(axis=0), columns=vectorizer.get_feature_names_out()).T
phrase_counts.columns = ['count']
print(phrase_counts.sort_values(by='count', ascending=False).head(10))
```

---
### Additional Tips
- Regularly refresh analysis with new data for continuous improvement.
- Combine quantitative and qualitative analyses for holistic insights.
